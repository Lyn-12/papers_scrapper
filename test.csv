abstract,author,cites,cites_id,journal,number,pages,publisher,title,url,volume,year
"The detectability and discriminability of virtual haptic gratings were analyzed in the frequency domain. Detection (Exp. 1) and discrimination (Exp. 2) thresholds for virtual haptic gratings were estimated using a force-feedback device that simulated sinusoidal and square-wave gratings with spatial periods from 0.2 to 38.4 mm. The detection threshold results indicated that for spatial periods up to 6.4 mm (i.e., spatial frequencies >0.156 cycle/mm), the detectability of square-wave gratings could be predicted quantitatively from the detection thresholds of their corresponding fundamental components. The discrimination experiment confirmed that at higher spatial frequencies, the square-wave gratings were initially indistinguishable from the corresponding fundamental components until the third harmonics were detectable. At lower spatial frequencies, the third harmonic components of square-wave gratings had lower …",Steven A Cholewiak and Kwangtaek Kim and Hong Z Tan and Bernard D Adelstein,54,18104797610932568627,IEEE Transactions on Haptics,1,3-14,IEEE,A frequency-domain analysis of haptic gratings,https://ieeexplore.ieee.org/abstract/document/5210096/,3,2009
"As haptics becomes an integral component of scientific data visualization systems, there is a growing need to study ""haptic glyphs"" (building blocks for displaying information through the sense of touch) and quantify their information transmission capability. The present study investigated the channel capacity for transmitting information through stiffness or force magnitude. Specifically, we measured the number of stiffness or force- magnitude levels that can be reliably identified in an absolute identification paradigm. The range of stiffness and force magnitude used in the present study, 0.2-3.0 N/mm and 0.1-5.0 N, respectively, was typical of the parameter values encountered in most virtual reality or data visualization applications. Ten individuals participated in a stiffness identification experiment, each completing 250 trials. Subsequently, four of these individuals and six additional participants completed 250 trials in a …",Steven A Cholewiak and Hong Z Tan and David S Ebert,51,9361504114905053519,,,87-91,IEEE,Haptic identification of stiffness and force magnitude,https://ieeexplore.ieee.org/abstract/document/4479918/,,2008
"Computer-graphics engineers and vision scientists want to generate images that reproduce realistic depth-dependent blur. Current rendering algorithms take into account scene geometry, aperture size, and focal distance, and they produce photorealistic imagery as with a high-quality camera. But to create immersive experiences, rendering algorithms should aim instead for perceptual realism. In so doing, they should take into account the significant optical aberrations of the human eye. We developed a method that, by incorporating some of those aberrations, yields displayed images that produce retinal images much closer to the ones that occur in natural viewing. In particular, we create displayed images taking the eye's chromatic aberration into account. This produces different chromatic effects in the retinal image for objects farther or nearer than current focus. We call the method ChromaBlur. We conducted two …",Steven A Cholewiak and Gordon D Love and Pratul P Srinivasan and Ren Ng and Martin S Banks,31,5738786554683183717,ACM Transactions on Graphics (TOG),6,1-12,ACM,ChromaBlur: Rendering chromatic eye aberration improves accommodation and realism,https://dl.acm.org/doi/abs/10.1145/3130800.3130815,36,2017
"Visual estimation of object stability is an ecologically important judgment that allows observers to predict the physical behavior of objects. A natural method that has been used in previous work to measure perceived object stability is the estimation of perceived “critical angle”—the angle at which an object appears equally likely to fall over versus return to its upright stable position. For an asymmetric object, however, the critical angle is not a single value, but varies with the direction in which the object is tilted. The current study addressed two questions:(a) Can observers reliably track the change in critical angle as a function of tilt direction?(b) How do they visually estimate the overall stability of an object, given the different critical angles in various directions? To address these questions, we employed two experimental tasks using simple asymmetric 3D objects (skewed conical frustums): settings of critical angle in different directions relative to the intrinsic skew of the 3D object (Experiment 1), and stability matching across 3D objects with different shapes (Experiments 2 and 3). Our results showed that (a) observers can perceptually track the varying critical angle in different directions quite well; and (b) their estimates of overall object stability are strongly biased toward the minimum critical angle (ie, the critical angle in the least stable direction). Moreover, the fact that observers can reliably match perceived object stability across 3D objects with different shapes suggests that perceived stability is likely to be represented along a single dimension.",Steven A Cholewiak and Roland W Fleming and Manish Singh,30,13400046667572762242,Journal of vision,4,12-12,The Association for Research in Vision and Ophthalmology,Visual perception of the physical stability of asymmetric three-dimensional objects,https://tvst.arvojournals.org/article.aspx?articleid=2121388,13,2013
"Inferring the mental states of other agents, including their goals and intentions, is a central problem in cognition. A critical aspect of this problem is that one cannot observe mental states directly, but must infer them from observable actions. To study the computational mechanisms underlying this inference, we created a two-dimensional virtual environment populated by autonomous agents with independent cognitive architectures. These agents navigate the environment, collecting “food” and interacting with one another. The agents’ behavior is modulated by a small number of distinct goal states: attacking, exploring, fleeing, and gathering food. We studied subjects’ ability to detect and classify the agents’ continually changing goal states on the basis of their motions and interactions. Although the programmed ground truth goal state is not directly observable, subjects’ responses showed both high validity (correlation …",Peter C Pantelis and Chris L Baker and Steven A Cholewiak and Kevin Sanik and Ari Weinstein and Chia-Chien Wu and Joshua B Tenenbaum and Jacob Feldman,29,13500426973403217301,Cognition,3,360-379,Elsevier,Inferring the intentional states of autonomous virtual agents,https://www.sciencedirect.com/science/article/pii/S0010027713002291,130,2014
"Humans can judge from vision alone whether an object is physically stable or not. Such judgments allow observers to predict the physical behavior of objects, and hence to guide their motor actions. We investigated the visual estimation of physical stability of 3-D objects (shown in stereoscopically viewed rendered scenes) and how it relates to visual estimates of their center of mass (COM). In Experiment 1, observers viewed an object near the edge of a table and adjusted its tilt to the perceived critical angle, ie, the tilt angle at which the object was seen as equally likely to fall or return to its upright stable position. In Experiment 2, observers visually localized the COM of the same set of objects. In both experiments, observers' settings were compared to physical predictions based on the objects' geometry. In both tasks, deviations from physical predictions were, on average, relatively small. More detailed analyses of individual observers' settings in the two tasks, however, revealed mutual inconsistencies between observers' critical-angle and COM settings. The results suggest that observers did not use their COM estimates in a physically correct manner when making visual judgments of physical stability.",Steven A Cholewiak and Roland W Fleming and Manish Singh,23,15736880631888070187,Journal of vision,2,13-13,The Association for Research in Vision and Ophthalmology,Perception of physical stability and center of mass of 3-D objects,https://jov.arvojournals.org/article.aspx?articleID=2213254,15,2015
"Blur occurs naturally when the eye is focused at one distance and an object is presented at another distance. Computer-graphics engineers and vision scientists often wish to create display images that reproduce such depth-dependent blur, but their methods are incorrect for that purpose. They take into account the scene geometry, pupil size, and focal distances, but do not properly take into account the optical aberrations of the human eye. We developed a method that, by incorporating the viewer's optics, yields displayed images that produce retinal images close to the ones that occur in natural viewing. We concentrated on the effects of defocus, chromatic aberration, astigmatism, and spherical aberration and evaluated their effectiveness by conducting experiments in which we attempted to drive the eye's focusing response (accommodation) through the rendering of these aberrations. We found that accommodation is not driven at all by conventional rendering methods, but that it is driven surprisingly quickly and accurately by our method with defocus and chromatic aberration incorporated. We found some effect of astigmatism but none of spherical aberration. We discuss how the rendering approach can be used in vision science experiments and in the development of ophthalmic/optometric devices and augmented-and virtual-reality displays.",Steven A Cholewiak and Gordon D Love and Martin S Banks,17,8341328198898590153,Journal of vision,9,1-1,The Association for Research in Vision and Ophthalmology,Creating correct blur and its effect on accommodation,https://jov.arvojournals.org/article.aspx?articleid=2701817,18,2018
"Two-interval two-alternative forced-choice discrimination experiments were conducted separately for sinusoidal and triangular textured surface gratings from which amplitude (i.e., height) discrimination thresholds were estimated. Participants (group sizes: n = 4 to 7) explored one of these texture types either by fingertip on real gratings (Finger real), by stylus on real gratings (Stylus real), or by stylus on virtual gratings (Stylus virtual). The real gratings were fabricated from stainless steel by an electrical discharge machining process while the virtual gratings were rendered via a programmable force-feedback device. All gratings had a 2.5-mm spatial period. On each trial, participants compared test gratings with 55, 60, 65, or 70 μm amplitudes against a 50-μm reference. The results indicate that discrimination thresholds did not differ significantly between sinusoidal and triangular gratings. With sinusoidal and triangular …",Matthew B Kocsis and Steven A Cholewiak and Ryan M Traylor and Bernard D Adelstein and E Daniel Hirleman and Hong Z Tan,11,16686586457504436047,IEEE transactions on haptics,2,181-192,IEEE,Discrimination of real and virtual surfaces with sinusoidal and triangular gratings using the fingertip and stylus,https://ieeexplore.ieee.org/abstract/document/6216374/,6,2012
"Two dilemmas arise in inferring shape information from shading. First, depending on the rendering physics, images can change significantly with (even) small changes in lighting or viewpoint, while the percept frequently does not. Second, brightness variations can be induced by material effects—such as pigmentation—as well as by shading effects. Improperly interpreted, material effects would confound shading effects. We show how these dilemmas are coupled by reviewing recent developments in shape inference together with a role for colour in separating material from shading effects. Aspects of both are represented in a common geometric (flow) framework, and novel displays of hue/shape interaction demonstrate a global effect with interactions limited to localized regions. Not all parts of an image are perceptually equal; shape percepts appear to be constructed from image anchor regions.",Benjamin Kunsberg and Daniel Holtmann-Rice and Emma Alexander and Steven Cholewiak and Roland Fleming and Steven W Zucker,9,3525044598165528785,Interface focus,4,20180019,The Royal Society,"Colour, contours, shading and shape: flow interactions reveal anchor neighbourhoods",https://royalsocietypublishing.org/doi/abs/10.1098/rsfs.2018.0019,8,2018
"Comprehension of goal-directed, intentional motion is an important but understudied visual function. To study it, we created a two-dimensional virtual environment populated by independently-programmed autonomous virtual agents, which navigate the environment, collecting food and competing with one another. Their behavior is modulated by a small number of distinct “mental states”: exploring, gathering food, attacking, and fleeing. In two experiments, we studied subjects’ ability to detect and classify the agents’ continually changing mental states on the basis of their motions and interactions. Our analyses compared subjects’ classifications to the ground truth state occupied by the observed agent’s autonomous program. Although the true mental state is inherently hidden and must be inferred, subjects showed both high validity (correlation with ground truth) and high reliability (correlation with one another). The data provide intriguing evidence about the factors that influence estimates of mental state—a key step towards a true “psychophysics of intention.”",Peter C Pantelis and Steven A. Cholewiak and Paul Ringstad and Kevin Sanik and Ari Weinstein and Chia-Chien Wu and Jacob Feldman,7,11164555767023932779,Journal of Vision,11,1990-1995,Association for Research in Vision and,Perception of intentions and mental states in autonomous virtual agents,https://escholarship.org/content/qt26s7519s/qt26s7519s.pdf,11,2011
"The interpretation of other agents as intentional actors equipped with mental states has been connected to the attribution of rationality to their behavior. But a workable definition of “rationality” is difficult to formulate in complex situations, where standard normative definitions are difficult to apply. In this study, we explore a notion of rationality based on the idea of evolutionary fitness. We ask whether agents that are more adapted to their environment are, consequently, perceived as more rational and intentional. We created a 2-D virtual environment populated with autonomous virtual agents, each of which behaves according to a built-in program equipped with simulated perception, memory, and decision making. We then introduced a process of simulated evolution that pressured the agents’ programs toward behavior more adapted to the simulated environment. We showed these agents to human subjects in 2 …",Peter C Pantelis and Timothy Gerstner and Kevin Sanik and Ari Weinstein and Steven A Cholewiak and Gaurav Kharkwal and Chia-Chien Wu and Jacob Feldman,6,6290421033584304174,Decision,1,40,Educational Publishing Foundation,Agency and rationality: Adopting the intentional stance toward evolved virtual agents.,https://psycnet.apa.org/journals/dec/3/1/40/,3,2016
"We examined the perception of virtual curved surfaces explored with a tool. We found a reliable curvature aftereffect, suggesting neural representation of the curvature in the absence of direct touch. Intermanual transfer of the aftereffect suggests that this representation is somewhat independent of the hand used to explore the surface.",Kristina Denisova and Melissa M. Kibbe and Steven A. Cholewiak and Sung-Ho Kim,6,3148891647529568450,IEEE Transactions on Haptics,,1,IEEE,Intra-and intermanual curvature aftereffect can be obtained via tool-touch,https://ieeexplore.ieee.org/abstract/document/6678361/,,2013
"The human eye changes focus—accommodates—to minimize blur in the retinal image. Previous work has shown that stimulation of nonfoveal retina can produce accommodative responses when no competing stimulus is presented to the fovea. In everyday situations it is very common for the fovea and other parts of the retina to be stimulated simultaneously. We examined this situation by asking how nonfoveal retina contributes to accommodation when the fovea is also stimulated. There were three experimental conditions.(a) Real change in which stimuli of different sizes, centered on the fovea, were presented at different optical distances. Accommodation was, as expected, robust because there was no conflicting stimulation of other parts of the retina.(b) Simulated change, no conflict in which stimuli of different sizes, again centered on the fovea, were presented at different simulated distances using rendered chromatic blur. Accommodation was robust in this condition because there was no conflict between the central and peripheral stimuli.(c) Simulated change, conflict in which a central disk (of different diameters) was presented along with an abutting peripheral annulus. The disk and annulus underwent opposite changes in simulated distance. Here we observed a surprisingly consistent effect of the peripheral annulus. For example, when the diameter of the central stimulus was 8 (thereby stimulating the fovea and parafovea), the abutting peripheral annulus had a significant effect on accommodation. We discuss how these results may help us understand other situations in which nonfixated targets affect the ability to focus on a fixated target …",Vivek Labhishetty and Steven A Cholewiak and Martin S Banks,5,1348845672647133078,Journal of Vision,12,18-18,The Association for Research in Vision and Ophthalmology,Contributions of foveal and non-foveal retina to the human eye's focusing response,https://jov.arvojournals.org/article.aspx?articleid=2753339,19,2019
"Augmented reality (AR) has recently gained momentum in the form of a variety of available optical see-through near-eye displays (NEDs) such as the Meta 2and the Microsoft Hololens. These devices are a big step forward towards Sutherland's vision of an ultimate display [Sutherland 1968]. The device we demonstrate attempts to deal with the main limitations of current devices. First, the graphics images are at a constant virtual distance for the eyes' accommodation mechanism, while the vergence of the two eyes working in concert places the virtual object (s) at a distance other than the accommodation distance. This vergence-accommodation conflict is one of the main problems in many AR and VR systems [Kress and Starner 2013]. The second limitation is achieving a wide FOV with compact optics. Cakmakci et al.[2006] contend that achieving a wide field-of-view (FOV) is the major optical design challenge in AR …",Kaan Akşit and Ward Lopes and Jonghyun Kim and Josef Spjut and Anjul Patney and Peter Shirley and David Luebke and Steven A Cholewiak and Pratul Srinivasan and Ren Ng and Martin S Banks and Gordon D Love,5,18130195908171454472,,,1-2,,Varifocal virtuality: a novel optical layout for near-eye display,https://dl.acm.org/doi/abs/10.1145/3084822.3084829,,2017
"Research on 3D shape has focused largely on the perception of local geometric properties, such as surface depth, orientation, or curvature. Relatively little is known about how the visual system organizes local measurements into global shape representations. Here, we investigated how the perceptual organization of shape affects the perception of physical stability of 3D objects. Estimating stability is important for predicting object behavior and guiding motor actions, and requires the observer to integrate information from the entire object.",Steven A Cholewiak and Manish Singh and Roland Fleming and Bina Pastakia,5,8373403526432059892,Journal of Vision,7,77-77,The Association for Research in Vision and Ophthalmology,The perception of physical stability of 3d objects: The role of parts,https://jov.arvojournals.org/article.aspx?articleid=2138901,10,2010
"The visual system can infer 3D shape from orientation flows arising from both texture and shading patterns. However, these two types of flows provide fundamentally different information about surface structure. Texture flows, when derived from distinct elements, mainly signal first-order features (surface slant), whereas shading flow orientations primarily relate to second-order surface properties. It is therefore crucial for the brain to identify whether flow patterns originate from shading or texture to correctly infer shape. One possible approach would be to use'surface appearance'(eg smooth gradients vs. fine-scale texture) to distinguish texture from shading. However, the structure of the flow fields themselves may indicate whether a given flow is more likely due to first-or second-order shape information. Here we test these two possibilities. We generated irregular objects ('blobs') using sinusoidal perturbations of …",Steven Cholewiak and Romain Vergne and Benjamin Kunsberg and Steven Zucker and Roland Fleming,4,16174097279596467563,Journal of vision,12,965-965,The Association for Research in Vision and Ophthalmology,Distinguishing between texture and shading flows for 3D shape estimation,https://jov.arvojournals.org/article.aspx?articleid=2434073,15,2015
"Perceiving 3D shape involves processing and combining different cues, including texture, shading, and specular reflections. We have previously shown that orientation flows produced by the various cues provide fundamentally different information about shape, leading to complementary strengths and weaknesses (see Cholewiak & Fleming, VSS 2013). An important consequence of this is that a given shape may appear different, depending on whether it is shaded or textured, because the different cues reveal different shape features. Here we sought to predict specific regions of interest (ROIs) within shapes where the different cues lead to better or worse shape perception. Since the predictions were derived from the orientation flows, our analysis provides a key test of how and when the visual system uses orientation flows to estimate shape. We used a gauge figure experiment to evaluate shape perception. Cues …",Steven A Cholewiak and Benjamin Kunsberg and Steven Zucker and Roland W Fleming,4,15050053021465917913,Journal of Vision,10,1113-1113,The Association for Research in Vision and Ophthalmology,Predicting 3D shape perception from shading and texture flows,https://jov.arvojournals.org/article.aspx?articleid=2144991,14,2014
"The tactile detectability of sinusoidal and square-wave virtual texture gratings were measured and analyzed. Using a three-interval one-up three-down adaptive tracking procedure, detection thresholds for virtual gratings were estimated using a custom-designed high position-resolution 3-degrees-of-freedom force-feedback haptic device. Two types of gratings were used, defined by sinusoidal and square waveforms, with spatial wavelengths of 0.2 to 25.6 mm. The results indicated that the participants demonstrated a higher sensitivity (i.e., lower detection threshold) to square-wave gratings than to sinusoidal ones at all the wavelengths tested. When the square-wave gratings were represented by the explicative Fourier series, it became apparent that the detectability of the square-wave gratings could be determined by that of the sinusoidal gratings at the corresponding fundamental frequencies. This was true for any …",Steven Cholewiak and Hong Z Tan,4,10022477494945159835,,,27-32,IEEE,Frequency analysis of the detectability of virtual haptic gratings,https://ieeexplore.ieee.org/abstract/document/4145146/,,2007
"The estimation of 3D shape from 2D images requires processing and combining many cues, including texture, shading, specular highlights and reflections. Previous research has shown that oriented filter responses ('orientation fields') may be used to perceptually reconstruct the surface structure of textured and shaded 3D objects. However, texture and shading provide fundamentally different information about 3D shape--texture provides information about surface orientations (which depend on the first derivative of the surface depth) while shading provides information about surface curvatures (which depend on higher derivatives). In this research project, we used specific geometric transformations that preserve the informativeness of one cue's orientation fields while disturbing the other cue’s orientation fields to investigate whether oriented filter responses predict the observed strengths and weaknesses of texture …",Steven A Cholewiak and Roland W Fleming,3,2012897025252602369,Journal of Vision,9,258-258,The Association for Research in Vision and Ophthalmology,Towards a unified explanation of shape from shading and texture,https://jov.arvojournals.org/article.aspx?articleid=2142365,13,2013
We developed a rendering method that takes into account the eye’s chromatic aberration. Accommodation is driven much more accurately with this method than with conventional methods. Perceived realism is also improved.,Martin S Banks and Steven A Cholewiak and Gordon D Love and Pratul Srinivasan and Ren Ng,2,13315911430222136838,,,DM2F. 1,Optical Society of America,ChromaBlur: Rendering Chromatic Eye Aberration Improves Accommodation and Realism in HMDs,https://www.osapublishing.org/abstract.cfm?uri=3d-2017-DM2F.1,,2017
"Blur occurs naturally when the eye is focused at one distance and an object is present at another distance. Vision scientists and computer-graphics (CG) engineers often wish to create display images that reproduce such depth-dependent blur, but their method is incorrect for that purpose. Their method appropriately takes into account the scene geometry, pupil size, and focus distances, but does not take into account the optical aberrations of the person who will view the resulting display images. We developed a method that, by incorporating the viewer's optics, yields displayed images that produce retinal images close to those in natural viewing. Here we concentrate on the effects of longitudinal chromatic aberration. This aberration creates different chromatic effects in the retinal image for object farther vs nearer than current focus. Our method handles this correctly. Observers viewed scenes with depth-dependent …",Steven Cholewiak and Gordon Love and Martin Banks,1,6006228010533872315,Journal of Vision,10,403-403,The Association for Research in Vision and Ophthalmology,Rendering correct blur,https://jov.arvojournals.org/article.aspx?articleid=2651286,17,2017
"The visual system can infer 3D shape from orientation flows arising from both texture and shading patterns. However, these two types of flows provide fundamentally different information about surface structure. Texture flows, when derived from distinct elements, mainly signal first-order features (surface slant), whereas shading flow orientations primarily relate to second-order surface properties (the change in surface slant).  The source of an image's structure is inherently ambiguous, it is therefore crucial for the brain to identify whether flow patterns originate from texture or shading to correctly infer shape from a 2D image. One possible approach would be to use 'surface appearance' (e.g. smooth gradients vs. fine-scale texture) to distinguish texture from shading. However, the structure of the flow fields themselves may indicate whether a given flow is more likely due to first- or second-order shape information. We test these two possibilities in this set of experiments, looking at speeded and free responses",Steven Cholewiak and Romain Vergne and Benjamin Kunsberg and Steven Zucker and Roland Fleming,1,3803486257224325801,,,,,Appearance controls interpretation of orientation flows for 3D shape estimation,https://hal.inria.fr/hal-01600027/,,2015
"Humans are generally remarkably good at inferring 3D shape from distorted patterns of reflections on mirror-like objects (Fleming et al, 2004). However, there are conditions in which shape perception fails (complex planar reliefs under certain illuminations; Faisman and Langer, 2013). A good theory of shape perception should predict failures as well as successes of shape perception, so here we sought to map out systematically the conditions under which subjects fail to estimate shape from specular reflections and to understand why. To do this, we parametrically varied the spatial complexity (spatial frequency content) of both 3D relief and illumination, and measured under which conditions subjects could and could not infer shape. Specifically, we simulated surface reliefs with varying spatial frequency content and rendered them as perfect mirrors under spherical harmonic light probes with varying frequency …",Julia Mazzarella and Steven Cholewiak and Flip Phillips and Roland Fleming,1,17673231121540755451,Journal of Vision,10,721-721,The Association for Research in Vision and Ophthalmology,Limits on the estimation of shape from specular surfaces,https://jov.arvojournals.org/article.aspx?articleid=2144595,14,2014
"Research on visual perception focuses primarily on the representation of overtly visible surface properties, eg, color, texture, orientation, curvature etc. In addition to estimating such properties—ie, representing what the objects in view are, and where they are located (eg, Marr, 1982)—an important goal of the visual system is to predict how visible objects are likely to behave in the near future. Predicting the physical behavior of objects is, among other things, crucial for the perceptual guidance of motor actions. Consider, for example, the visual guidance of motor actions aimed at intercepting an object in motion, or at catching a precariously placed object that is about to fall. Predicting the physical behavior of objects in these, and other, situations requires observers to infer hidden forces acting on objects, eg, gravity, support, friction, etc.—and often to do so from vision alone.Research on intuitive physics has shown that people often hold erroneous physical intuitions. For example, many people expect that a ball being swung at the end of a string will, if the string breaks, fly off along a curved trajectory; or that an object dropped from a flying airplane will fall vertically straight down (eg, McCloskey et al., 1980). On the other hand, our visuo-motor interactions with objects in everyday life suggest that we usually have a good comprehension of physical attributes such as gravity, friction, and support relations. Indeed, subsequent research has shown that people are much more sensitive to violations of physical laws when they view real-time dynamic displays, rather than when explicitly asked about their intuitions (Kaiser et",Steven A. Cholewiak,1,10966284922852760188,,,,,The tipping point: Visual estimation of the physical stability of three-dimensional objects,https://pdfs.semanticscholar.org/45b3/3ae84ddccb289da0d6808fc18383230ae680.pdf,,2013
"Visual estimation of object stability is an ecologically important judgment that allows observers to predict the physical behavior of objects. A natural method that has been used in previous work to measure perceived object stability is the estimation of perceived “critical angle”–the angle at which an object appears equally likely to fall over versus return to its upright stable position. For an asymmetric object, however, the critical angle is not a single value, but varies with the direction in which the object is tilted. The current study addressed two questions:(1) Can observers reliably track the change in critical angle as a function of tilt direction?(2) How do they visually estimate the overall stability of an object, given the different critical angles in various directions? To address these questions, we employed two experimental tasks using simple asymmetric 3D objects (skewed conical frustums): settings of critical angle in different directions relative to the intrinsic skew of the 3D object (Experiment 1), and stability matching across 3D objects with different shapes (Experiments 2 & 3). Our results showed that (1) Observers can perceptually track the varying critical angle in different directions quite well; and (2) Their estimates of overall object stability are strongly biased toward the minimum critical angle (ie, the critical angle in the least stable direction). Moreover, the fact that observers can reliably match perceived object stability across different 3D shapes suggests that perceived stability is likely to be represented along a single dimension.",Steven A Cholewiak and Manish Singh and Roland W. Fleming,1,16519466829062962063,Journal of Vision,11,44-44,Association for Research in Vision and,Perception of Physical Stability of Asymmetrical Three-Dimensional Objects,https://pdfs.semanticscholar.org/c4f6/3f2a2f31b0fe46fdf98483323d4a47ab1f77.pdf,11,2011
,Roger W. Cholewiak and Steven A. Cholewiak,1,1034832298898815382,,,343–348,SAGE,Cutaneous perception,http://scholar.google.com/scholar?cluster=1034832298898815382&hl=en&oi=scholarr,1,2010
"Conventional wisdom is that accommodation in humans exhibits significant errors. When the stimulus is far, the eye is thought to focus too near (“lead of accommodation”). When the stimulus is near, it focuses too far (“lag”). These errors are as large as 1 diopter, which should produce noticeably blurred imagery. But viewers typically do not experience the blur expected from such leads and lags. We re-examined this phenomenon by measuring accommodation objectively and subjectively. Objective measurements are based on measurements of light reflected off the retina. Subjective measurements are based on the viewer performing a visual task; they are more valid because they involve the whole visual process. We used a custom varifocal display apparatus to present accommodative stimuli to six young adults. On each trial, subjects fixated and focused on a Maltese cross at a distance of 0, 2, 4 or 6D. A …",Martin Banks and Vivek Labhishetty and Steven Cholewiak,0,,Journal of Vision,11,1650-1650,The Association for Research in Vision and Ophthalmology,Lags and leads of accommodation: Fact or fiction?,https://jov.arvojournals.org/article.aspx?articleid=2771322,20,2020
"Creating immersive 3D stereoscopic, autostereoscopic, and lightfield experiences are becoming the center point of optical design of future head mounted displays and lightfield displays. However, despite the advancement in 3D and light field displays; there is no consensus on what are the necessary quantized depth levels for such emerging displays at stereoscopic or monocular modalities. Here we start from psychophysical theories and work toward defining and prioritizing quantized levels of depth that would saturate the human depth perception. We propose a general optimization framework, which locates the depth levels in a\emph {globally optimal} way for band limited displays. While the original problem is computationally intractable, we manage to find a tractable reformulation as maximally covering a region of interest with a selection of hypographs corresponding to the monocular depth of field profiles. The results show that on average 1731 stereoscopic and 8 monocular depth levels (distributed from 25 cm to infinity) would saturate the visual depth perception. Also the first 3 depth levels should be allocated at (148), then (83, 170), then (53, 90, 170) distances respectively from the face plane to minimize the monocular error in the entire population. The study further discusses the 3D spatial profile of the quantized stereoscopic and monocular depth levels. The study provides fundamental guidelines for designing optimal near eye displays, light-field monitors, and 3D screens.",Alireza Aghasi and Barmak Heshmat and Leihao Wei and Moqian Tian and Steven A Cholewiak,0,,arXiv preprint arXiv:2010.06382,,,,Optimal Allocation of Quantized Human Eye Depth Perception for Light Field Display Design,https://arxiv.org/abs/2010.06382,,2020
"Purpose: Typically, objective measurement of accommodation using autorefractors, photorefractors or aberrometers reveals response lags and leads implying that the eye does not focus precisely at the distance of the object of regard. These errors are thought to be due to the eye’s depth of focus. Because of the depth of focus an out-of-focus image is perceived as acceptably sharp. We examined these response errors by comparing objective and subjective measures of best focus.Methods: Accommodative stimuli were presented to six healthy adults (20-35 years) using a varifocal display system. Changes in accommodation and pupil size were recorded using a wavefront sensor. On each trial, subjects first fixated a Maltese cross at a distance of 0, 2, 4 or 6D and were told to try to maintain image sharpness. After the 3-sec presentation of the cross, we measured visual acuity with a briefly presented tumbling E …",Vivek Labhishetty and Steven A Cholewiak and Martin S Banks,0,,Investigative Ophthalmology & Visual Science,7,1717-1717,The Association for Research in Vision and Ophthalmology,Lags and leads of accommodation are smaller than previously thought,https://iovs.arvojournals.org/article.aspx?articleid=2767180,61,2020
"In optics in general, a sharp aberration-free image is normally the desired goal, and the whole field of adaptive optics has developed with the aim of producing blur-free images. Likewise, in ophthalmic optics we normally aim for a sharp image on the retina. But even with an emmetropic, or well-corrected eye, chromatic and high order aberrations affect the image. We describe two different areas where it is important to take these effects into account and why creating blur correctly via rendering can be advantageous. Firstly we show how rendering chromatic aberration correctly can drive accommodation in the eye and secondly report on matching defocus-l generated using rendering with conventional optical defocus.",Gordon D Love and Martin S Banks and Steven A Cholewiak and Abigail P Finch,0,,,,1124816,International Society for Optics and Photonics,Creating correct aberrations: why blur isn’t always bad in the eye,https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11248/1124816/Creating-correct-aberrations--why-blur-isnt-always-bad-in/10.1117/12.2553964.short,11248,2020
"In computer-generated imagery and vision science, defocus blur is often rendered to simulate objects closer or farther than the focal plane. But depth-dependent optical effects, like longitudinal chromatic aberration (LCA), are not implemented in a physically correct manner. Recent evidence has shown that incorporating LCA into rendered images produces a powerful cue for driving accommodation and depth perception. But implementing correct LCA effects is computationally expensive. Applied implementations of defocus blur with LCA are possible, but require approximations in order to run in real-time. We investigated whether real-time implementation of blur with LCA using approximate blur kernels and simplified treatment of occlusions can still drive accommodation and improve perceived depth compared to conventional methods that do not incorporate LCA. We measured accommodative responses with an …",Steven A Cholewiak and Peter Shirley and Morgan McGuire and Martin S Banks,0,,Journal of Vision,10,15b-15b,The Association for Research in Vision and Ophthalmology,Real-time blur with chromatic aberration drives accommodation and depth perception,http://scholar.google.com/scholar?cluster=17591595053098376980&hl=en&oi=scholarr,19,2019
"Purpose: Experiments suggest that hyperopic focus in the peripheral retina increases the risk of developing myopia. This implies that signed defocus can trigger eye growth. But does this mean blur in the periphery affects other oculomotor mechanisms such as accommodation? Stimulation of the human peripheral retina when no foveal stimulus is present elicits accommodation. But in natural viewing, the fovea and periphery are nearly always both stimulated. We investigated accommodative responses when the fovea and periphery are both stimulated, but with different signs of defocus.Methods: 10 participants (18-25 yrs) viewed black-white textures monocularly. The stimuli varied in depth (±1.5 D) sinusoidally (0.1, 0.2, 0.5, or 1.0 Hz). Three conditions were tested: Real Blur, Defocus+ LCA, and Defocus+ LCA Conflict. In Real Blur, the optical distance of the textures (disks subtending 1, 2, 4, 6, 8, or 14) was varied …",Vivek Labhishetty and Steven Cholewiak and Martin S Banks,0,,Investigative Ophthalmology & Visual Science,9,1402-1402,The Association for Research in Vision and Ophthalmology,Peripheral stimulation can override foveal stimulation in driving accommodation,http://scholar.google.com/scholar?cluster=2962272328724751754&hl=en&oi=scholarr,60,2019
"A pseudo light-field display uses a stereoscopic display viewed by a user, with a variable lens disposed between each eye and the display, and a half-silvered mirror disposed between each lens and the display. A focus measurement device operates through at least one half-silvered mirror with one of the variable lenses to detect focus of an eye, providing a focus output, and controlling both variable lenses. A gaze direction measurement device operates through both half-silvered mirrors to detect the gaze direction of each eye, and provides an output of the vergence or individual gaze directions of the eyes. The focus, vergence, and gaze directions are used to establish a visual focal plane, whereby objects on the display that are being gazed upon in the visual focal plane are in focus, with other objects appropriately blurred, thereby approximating a light-field display.",,0,,,,,,Pseudo light-field display apparatus,https://patents.google.com/patent/US20190137758A1/en,,2019
"Retinal-image blur occurs when the eye is focused at one distance and an object is at another. Vision scientists and computer-graphics engineers often wish to create images that reproduce such depth-dependent blur, but their method is incorrect because it does not incorporate the human eye's optical aberrations. We developed a rendering method that, by incorporating these aberrations, creates displayed images that produce more natural retinal images. Here we concentrate on one aberration: longitudinal chromatic aberration (LCA). LCA creates different chromatic effects in the retinal image for objects farther vs nearer than current focus. We asked whether one can drive eye focus (accommodation) by incorporating LCA into the rendering of objects meant to appear farther or nearer than current focus. Observers viewed textured planes monocularly in three conditions: 1) Real Change in which stimulus focal …",Martin Banks and Steven Cholewiak and Gordon Love,0,,Journal of Vision,10,582-582,The Association for Research in Vision and Ophthalmology,ChromaBlur: Rendering natural chromatic aberration drives accommodation effectively,https://jov.arvojournals.org/article.aspx?articleid=2699572,18,2018
"The purpose of accommodation is to minimize blur. Defocus blur is the major source of blurring in the retinal image, but defocus blur itself cannot tell the eye if it is focused too near or too far. When the human eye is shown real blur, accommodation always changes in the correct direction without searching, so the visual system can somehow determine the sign of defocus. Potential signals for the sign include temporal fluctuations of accommodation (eg, microfluctuations), chromatic aberration, and higher-order aberrations (HOAs). We investigated whether simulated HOAs—specifically, astigmatism and spherical aberration—provide the needed sign information to drive accommodation in the right direction. Measurable astigmatism occurs in most people; its magnitude and axis varies across individuals. The point-spread function (PSF) of a defocused astigmatic eye is elliptical with the major axis in one direction when …",Steven Cholewiak and Gordon Love and Martin Banks,0,,Journal of Vision,10,500-500,The Association for Research in Vision and Ophthalmology,Driving accommodation using simulated higher-order aberrations,https://jov.arvojournals.org/article.aspx?articleid=2699491,18,2018
"Accommodation is the process by which the eye lens changes optical power to maintain a clear retinal image as the distance to the fixated object varies. Although luminance blur has long been considered the driving feature for accommodation, it is by definition unsigned (ie, there is no difference between the defocus of an object closer or farther than the focus distance). Nonetheless, the visual system initially accommodates in the correct direction, implying that it exploits a cue with sign information. Here, we present a model of accommodation control based on such a cue: Longitudinal Chromatic Aberration (LCA). The model relies on color-opponent units, much like those observed among retinal ganglion cells, to make the computation required to use LCA to drive accommodation.",Agostino Gibaldi and Steven A Cholewiak and Marty S Banks,0,,,,,,Modeling Accommodation Control of the Human Eye: Chromatic Aberration and Color Opponency,https://docs.lib.purdue.edu/modvis/2017/session03/2/,,2017
"Cracks, crevices and other surface concavities are typically dark places where both dirt and shadows tend to get trapped. By contrast, convex features are exposed to light and often get buffed a lighter or more glossy shade through contact with other surfaces. This means that in many cases, for complex surface geometries, shading and pigmentation are spatially correlated with one another, with dark concavities that are dimly illuminated and lighter convexities, which are more brightly shaded. How does the visual system distinguish between pigmentation and shadows when the two are spatially correlated? We performed a statistical analysis of complex rough surfaces under illumination conditions that varied parametrically from highly directional to highly diffuse in order to characterise the relationships between shading, illumination and shape. Whereas classical shape from shading analyses relate image …",Roland Fleming and Steven Cholewiak,0,,Journal of Vision,10,1317-1317,The Association for Research in Vision and Ophthalmology,The Dark Secrets of Dirty Concavities,https://jov.arvojournals.org/article.aspx?articleid=2145195,14,2014
,Steven A. Cholewiak and Gizem Küçükoğlu,0,,,,,,What happens to a shiny 3D object in a rotating environment?,,,2014
,Julia E. Mazzarella and Steven A. Cholewiak and Flip Phillips and Roland W. & Fleming,0,,,,,,Effects of varied spatial scale on perception of shape from shiny surfaces,,,2014
,Roland W. Fleming and Steven A. Cholewiak,0,,,,,,Visually disentangling shading and surface pigmentation when the two are correlated,,,2014
,Steven A. Cholewiak and Benjamin Kunsberg and Steven Zucker and Roland W. Fleming,0,,,,,,Perceptual regions of interest for 3D shape derived from shading and texture flows,,,2014
"Visual estimation of object stability is an ecologically important judgment that allows observers to predict the physical behavior of objects. A natural method that has been used in previous work to measure perceived object stability is the estimation of perceived ‘‘critical angle’’—the angle at which an object appears equally likely to fall over versus return to its upright stable position. For an asymmetric object, however, the critical angle is not a single value, but varies with the direction in which the object is tilted. The current study addressed two questions:(a) Can observers reliably track the change in critical angle as a function of tilt direction?(b) How do they visually estimate the overall stability of an object, given the different critical angles in various directions? To address these questions, we employed two experimental tasks using simple asymmetric 3D objects (skewed conical frustums): settings of critical angle in different directions relative to the intrinsic skew of the 3D object (Experiment 1), and stability matching across 3D objects with different shapes (Experiments 2 and 3). Our results showed that (a) observers can perceptually track the varying critical angle in different directions quite well; and (b) their estimates of overall object stability are strongly biased toward the minimum critical angle (ie, the critical angle in the least stable direction). Moreover, the fact that observers can reliably match perceived object stability across 3D objects with different shapes suggests that perceived stability is likely to be represented along a single dimension.",SA Cholewiak and RW Fleming and M Singh,0,,,,,,Visual perception of the physical stability of asymmetric three,http://www.academia.edu/download/47945122/Visual_perception_of_the_physical_stabil20160810-8946-1oqjtib.pdf,,2013
"Physical stability is an ecologically important judgment about objects that allows observers to predict object behavior and appropriately guide motor actions to interact with them. Moreover, it is an attribute that observers can estimate from vision alone, based on an object's shape. In previous work, we have used different tasks to investigate perceived object stability, including estimation of critical angle of tilt and matching stability across objects with different shapes (VSS 2010, 2011). The ability to perform these tasks, however, does not necessarily indicate that object stability is a natural perceptual dimension. We asked whether it is possible to obtain adaptation aftereffects with perceived object stability. Does being exposed to a sequence of highly stable (bottom-heavy) objects make a test object appear less stable (and vice versa)? Our test objects were vertically elongated shapes, with a flat base and a vertical axis …",Steven A Cholewiak and Manish Singh and Roland Fleming,0,,Journal of Vision,9,304-304,The Association for Research in Vision and Ophthalmology,Visual adaptation to physical stability of objects,https://jov.arvojournals.org/article.aspx?articleid=2140951,12,2012
"Repeated haptic exploration of a surface with curvature results in an adaptation effect, such that flat surfaces feel curved in the opposite direction of the explored surface. Previous studies used real objects and involved contact of skin on surface with no visual feedback. To what extent do cutaneous, proprioceptive, and visual cues play a role in the neural representation of surface curvature? The current study used a Personal Haptic Interface Mechanism (PHANToM) force-feedback device to simulate physical objects that subjects could explore with a stylus. If haptic aftereffect is observed in exploration of virtual surfaces, it suggests neural representations of curvature based solely on proprioceptive input. If visual input plays a role in the absence of haptic convexity/concavity, it would provide evidence for a visual input to the neural haptic representation. Method. Baseline curvature discrimination was obtained from …",Melissa M Kibbe and Sung-Ho Kim and Steven Cholewiak and Kristina Denisova,0,,Journal of Vision,11,784-784,The Association for Research in Vision and Ophthalmology,Curvature aftereffect and visual-haptic interactions in simulated environments,https://jov.arvojournals.org/article.aspx?articleid=2140333,11,2011
"Visual estimation of object stability is an ecologically important judgment that allows observers to predict objects' physical behavior. Previously we have used the'critical angle'of tilt to measure perceived object stability (VSS 2010; VSS 2011). The current study uses a different paradigm—measuring the critical extent to which an object can"" stick out"" over a precipitous edge before falling off. Observers stereoscopically viewed a rendered scene containing a 3D object placed near a table’s edge. Objects were slanted conical frustums that varied in their slant angle, aspect ratio, and direction of slant (pointing toward/away from the edge). In the stability task, observers adjusted the horizontal position of the object relative to the precipitous edge until it was perceived to be in unstable equilibrium. In the CoM task, they adjusted the height of a small ball probe to indicate perceived CoM. Results exhibited a significant effect of …",Steven A. Cholewiak and Roland W. Fleming and Manish Singh,0,,,,101,PION LTD,On the edge: Perceived stability and center of mass of 3D objects,http://scholar.google.com/scholar?cluster=15927746634743716247&hl=en&oi=scholarr,40,2011
,Peter Pantelis and Steven A Cholewiak and Tim Gerstner and Gaurav Kharkwal and Kevin Sanik and Ari Weinstein and Chia-Chien Wu and Jacob Feldman,0,,,,,,Perceiving intelligent action: Experiments in the interpretation of intentional motion,,,2011
"This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during 2010, and items from previous years that were commented upon or corrected in 2010. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the coauthors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.",Bernard D Adelstein and Teemu Ahmaniemi and Karlin Bark and Wouter M Bergmann Tiest and Matteo Bianchi and Antonio Bicchi and Gianni Borghesan and Martin Buss and Lihan Chen and Seungmoon Choi and Steven A Cholewiak and Erik C Chubb and Jaemin Chun and J Edward Colgate and Juan Manuel Cruz-Hernandez and Mark Cutkosky and Jenny Dankelman and Massimiliano Di Luca and Maaike Duistermaat and Nathaniel I Durlach and Kaveh Elizeh and Linda R Elliott and Marc O Ernst and Thomas K Ferris and Brian T Gleeson and Richard HM Goossens and Danny Grant and Giorgio Grioli and Amir Haddadi and Cheryl Hamilton and Sung H Han and Matthias Harders and Keyvan Hashtrudi-Zaad and Sandra Hirche and Scott K Horschel and Takayuki Hoshi and Takayuki Iwamoto and Jack J Jakimowicz and Lynette Jones,0,,IEEE Transactions on Haptics,4,1,,2010 Index IEEE Transactions on Haptics Vol. 3,https://ieeexplore.ieee.org/abstract/document/5725223/,3,2010
,Steven A Cholewiak and Peter Pantelis and Paul Ringstad and Kevin Sanik and Ari Weinstein and Chia-Chien Wu and Jacob Feldman,0,,,,,,Living within a virtual environment populated by intelligent autonomous agents,,,2010
,Roger W. Cholewiak and Steven A. Cholewiak,0,,,,722–726,SAGE,Pain: Physiological mechanisms,,1,2010
"Given the prevalence of ambiguity and noise in the environment, and the inductive nature of inferences in perception and cognition, most visual, visuo-motor, and cognitive tasks necessarily involve noisy or probabilistic representations. Whether the perceptual system is tasked with estimating scene parameters from images (Knill, Kersten & Yuille, 1996; Geisler & Kersten, 2002; Mamassian, Landy & Maloney, 2003) or to generate non-verbal numerical estimates (Burr & Ross, 2008; Cordes, Gallistel, Gelman & Latham, 2007), all representations involve uncertainty. Veridical representations of scene parameters would be a hindrance in a changing environment unless one could use past and present perceptual information in a dynamic manner, embracing the innate variability of the real-world. The fact that individuals have little difficulty in performing perceptual and cognitive tasks and can make judgments in a computationally optimal manner suggests that there may be explicit representations of uncertainty and variance in the perceptual system, which provides a wealth of information for planning future actions in an uncertain world.All perceptual representations have local and global parameters associated with elements of the environment. Perceptual and cognitive systems need to be able to make computations with these representations in order to plan actions and to predict future events. Parameter estimates alone (whether individual values or means) are insufficient for most computations, which necessarily rely on variance information in a number of critical ways. For example, cue combination within and across modalities (eg, visual/haptic …",Steven A. Cholewiak and Manish Singh,0,,Journal of Vision,8,1019,,Perceptual estimation of variance in orientation and its dependence on sample size,https://rucore.libraries.rutgers.edu/rutgers-lib/27431/,9,2009
,Steven A. Cholewiak and Hong Z. Tan,0,,,,83,,Haptic stiffness identification and information transfer,,,2007
"The interpretation of other agents as intentional actors equipped with mental states has been connected to the attribution of rationality to their behavior. But a workable definition of “rationality” is difficult to formulate in complex situations, where standard normative definitions are difficult to apply. In this paper, we explore a notion of rationality based on the idea of evolutionary fitness. We ask whether agents that are more adapted to their environment are, consequently, perceived as more rational and intentional. We created a 2-D virtual environment populated with autonomous virtual agents, each of which behaves according to built-in programs equipped with simulated perception, memory, and decision making. We then introduced a process of simulated evolution that pressured the agents’ programs towards behavior more adapted to the simulated environment. We showed these agents to human subjects in two …",Peter C Pantelis and Timothy Gerstner and Kevin Sanik and Ari Weinstein and Steven A Cholewiak and Gaurav Kharkwal and Chia-Chien Wu and Jacob Feldman,0,,,,,,This manuscript has been accepted for publication at Decision,http://scholar.google.com/scholar?cluster=7155571917089501426&hl=en&oi=scholarr,,
